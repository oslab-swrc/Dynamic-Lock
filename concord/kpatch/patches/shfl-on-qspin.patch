diff --git a/kernel/bpf/inode.c b/kernel/bpf/inode.c
index a70f7209c..4663fa354 100644
--- a/kernel/bpf/inode.c
+++ b/kernel/bpf/inode.c
@@ -701,3 +701,61 @@ static int __init bpf_init(void)
 	return ret;
 }
 fs_initcall(bpf_init);
+
+
+#include "kpatch-macros.h"
+
+static inline __u64 ptr_to_u64(const void *ptr)
+{
+    return (__u64) (unsigned long) ptr;
+}
+
+static void *get_pinned_bpf_obj(const char *pathname){
+	struct inode *inode;
+	struct path path;
+	void *raw;
+	int ret;
+
+	/* Let's get BPF prog 1 */
+	ret = kern_path(pathname, LOOKUP_FOLLOW, &path);
+	if (ret){
+		printk("[concord] %s failed\n", pathname);
+		return ERR_PTR(ret);
+	}
+
+	inode = d_backing_inode(path.dentry);
+	ret = inode_permission(inode, ACC_MODE(2));
+	if(ret){
+		printk("[concord] perm error\n");
+		path_put(&path);
+		return ERR_PTR(ret);
+	}
+
+	raw = bpf_any_get(inode->i_private, BPF_TYPE_PROG);
+	if(!IS_ERR(raw)){
+		touch_atime(&path);
+	}
+	else{
+		printk("[concord] raw error\n");
+		path_put(&path);
+		return ERR_PTR(ret);
+	}
+
+	path_put(&path);
+	return raw;
+}
+
+static int pre_patch_callback(patch_object *obj)
+{
+	extern void *bpf_prog_custom_cmp_func;
+
+	bpf_prog_custom_cmp_func = get_pinned_bpf_obj("/sys/fs/bpf/numa-grouping");
+	if(IS_ERR(bpf_prog_custom_cmp_func)){
+		printk("[concord] bpf_policy failed\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+KPATCH_PRE_PATCH_CALLBACK(pre_patch_callback);
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index b8dfc3498..ba67da07a 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -29,6 +29,9 @@
  * Include queued spinlock statistics code
  */
 #include "qspinlock_stat.h"
+#include <linux/lock_policy.h>
+#include <linux/filter.h>
+#include <linux/livepatch.h>
 
 /*
  * The basic principle of a queue-based spinlock can best be understood
@@ -187,7 +190,17 @@ static bool numa_cmp_func(struct qspinlock* lock, struct mcs_spinlock *node, str
 
 static bool custom_lock_cmp(struct qspinlock* lock, struct mcs_spinlock *node, struct mcs_spinlock *curr)
 {
-	return false;
+	extern void *bpf_prog_custom_cmp_func;
+	struct bpf_prog *prog;
+	prog = bpf_prog_custom_cmp_func;
+
+	struct lock_policy_args args;
+	args.nid1 = node->nid;
+	args.nid2 = curr->nid;
+
+	int ret = BPF_PROG_RUN(prog, &args);
+
+	return ret;
 }
 
 static int custom_lock_acquire(struct qspinlock *lock)
@@ -233,7 +246,7 @@ static void shuffle_waiters(struct qspinlock *lock, struct mcs_spinlock *node,
 	shuffler_cmp_func cmp;
 
 	if(custom){
-		cmp = &numa_cmp_func;
+		cmp = &custom_lock_cmp;
 	}
 	else{
 		cmp = &numa_cmp_func;
@@ -975,7 +988,7 @@ void queued_spin_lock(struct qspinlock *lock)
 	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
 		return;
 
-	queued_spin_lock_slowpath(lock, val, 0);
+	queued_spin_lock_slowpath(lock, val, 1);
 }
 EXPORT_SYMBOL(queued_spin_lock);
 
